@misc{shuklasingularity,
      title={Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads}, 
      author={Dharma Shukla and Muthian Sivathanu and Srinidhi Viswanatha and Bhargav Gulavani and Rimma Nehme and Amey Agrawal and Chen Chen and Nipun Kwatra and Ramachandran Ramjee and Pankaj Sharma and Atul Katiyar and Vipul Modi and Vaibhav Sharma and Abhishek Singh and Shreshth Singhal and Kaustubh Welankar and Lu Xun and Ravi Anupindi and Karthik Elangovan and Hasibur Rahman and Zhou Lin and Rahul Seetharaman and Cheng Xu and Eddie Ailijiang and Suresh Krishnappa and Mark Russinovich},
      year={2022},
      eprint={2202.07848},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{zero,
  author       = {Samyam Rajbhandari and
                  Jeff Rasley and
                  Olatunji Ruwase and
                  Yuxiong He},
  title        = {ZeRO: Memory Optimization Towards Training {A} Trillion Parameter
                  Models},
  journal      = {CoRR},
  volume       = {abs/1910.02054},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.02054},
  eprinttype    = {arXiv},
  eprint       = {1910.02054},
  timestamp    = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-02054.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{rajbhandari2021zero-infinity,
author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
title = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
howpublished = {arXiv},
year = {2021},
month = {April},
abstract = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16~GB to 80~GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model.

In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40\% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed.ai.},
url = {https://www.microsoft.com/en-us/research/publication/zero-infinity-breaking-the-gpu-memory-wall-for-extreme-scale-deep-learning/},
}

@misc{jiang2023osdp,
      title={OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning}, 
      author={Youhe Jiang and Fangcheng Fu and Xupeng Miao and Xiaonan Nie and Bin Cui},
      year={2023},
      eprint={2209.13258},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
}

@article{10.1145/2817817.2731192,
author = {Kehne, Jens and Metter, Jonathan and Bellosa, Frank},
title = {GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/2817817.2731192},
doi = {10.1145/2817817.2731192},
abstract = {Over the last few years, GPUs have been finding their way into cloud computing platforms, allowing users to benefit from the performance of GPUs at low cost. However, a large portion of the cloud's cost advantage traditionally stems from oversubscription: Cloud providers rent out more resources to their customers than are actually available, expecting that the customers will not actually use all of the promised resources. For GPU memory, this oversubscription is difficult due to the lack of support for demand paging in current GPUs. Therefore, recent approaches to enabling oversubscription of GPU memory resort to software scheduling of GPU kernels -- which has been shown to induce significant runtime overhead in applications even if sufficient GPU memory is available -- to ensure that data is present on the GPU when referenced.In this paper, we present GPUswap, a novel approach to enabling oversubscription of GPU memory that does not rely on software scheduling of GPU kernels. GPUswap uses the GPU's ability to access system RAM directly to extend the GPU's own memory. To that end, GPUswap transparently relocates data from the GPU to system RAM in response to memory pressure. GPUswap ensures that all data is permanently accessible to the GPU and thus allows applications to submit commands to the GPU directly at any time, without the need for software scheduling. Experiments with our prototype implementation show that GPU applications can still execute even with only 20 MB of GPU memory available. In addition, while software scheduling suffers from permanent overhead even with sufficient GPU memory available, our approach executes GPU applications with native performance.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {65–77},
numpages = {13},
keywords = {memory overcommitment, swapping, virtualization, oversubscription, gpu}
}

@inproceedings{gpuswap,
author = {Kehne, Jens and Metter, Jonathan and Bellosa, Frank},
title = {GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping},
year = {2015},
isbn = {9781450334501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2731186.2731192},
doi = {10.1145/2731186.2731192},
abstract = {Over the last few years, GPUs have been finding their way into cloud computing platforms, allowing users to benefit from the performance of GPUs at low cost. However, a large portion of the cloud's cost advantage traditionally stems from oversubscription: Cloud providers rent out more resources to their customers than are actually available, expecting that the customers will not actually use all of the promised resources. For GPU memory, this oversubscription is difficult due to the lack of support for demand paging in current GPUs. Therefore, recent approaches to enabling oversubscription of GPU memory resort to software scheduling of GPU kernels -- which has been shown to induce significant runtime overhead in applications even if sufficient GPU memory is available -- to ensure that data is present on the GPU when referenced.In this paper, we present GPUswap, a novel approach to enabling oversubscription of GPU memory that does not rely on software scheduling of GPU kernels. GPUswap uses the GPU's ability to access system RAM directly to extend the GPU's own memory. To that end, GPUswap transparently relocates data from the GPU to system RAM in response to memory pressure. GPUswap ensures that all data is permanently accessible to the GPU and thus allows applications to submit commands to the GPU directly at any time, without the need for software scheduling. Experiments with our prototype implementation show that GPU applications can still execute even with only 20 MB of GPU memory available. In addition, while software scheduling suffers from permanent overhead even with sufficient GPU memory available, our approach executes GPU applications with native performance.},
booktitle = {Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {65–77},
numpages = {13},
keywords = {swapping, virtualization, gpu, memory overcommitment, oversubscription},
location = {Istanbul, Turkey},
series = {VEE '15}
}



@Misc{accelerate,
  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year =         {2022},
}

@inproceedings{deepspeed,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {distributed deep learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@INPROCEEDINGS{unified,
  author={Li, Wenqiang and Jin, Guanghao and Cui, Xuewen and See, Simon},
  booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, 
  title={An Evaluation of Unified Memory Technology on NVIDIA GPUs}, 
  year={2015},
  volume={},
  number={},
  pages={1092-1098},
  doi={10.1109/CCGrid.2015.105},
}

